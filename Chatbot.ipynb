{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanMr7/ChatbotIA/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBdu1qF7RA7o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout, LayerNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Descargar el DataSet en Colab\n",
        "url_en = \"https://github.com/JuanMr7/ProyectoIAChatBot/raw/main/europarl-v7.es-en.en\"\n",
        "url_es = \"https://github.com/JuanMr7/ProyectoIAChatBot/raw/main/europarl-v7.es-en.es\"\n",
        "\n",
        "urllib.request.urlretrieve(url_en, \"europarl-v7.es-en.en\")\n",
        "urllib.request.urlretrieve(url_es, \"europarl-v7.es-en.es\")\n",
        "\n",
        "# Leer los archivos descargados\n",
        "with open(\"europarl-v7.es-en.en\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "\n",
        "with open(\"europarl-v7.es-en.es\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_es = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9rg8eVJyRB",
        "outputId": "cb32daa9-748b-41eb-9c91-14a58ede8d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFNO1-hizpBb"
      },
      "outputs": [],
      "source": [
        "#Limpieza de datos ingles\n",
        "corpus_en = europarl_en\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "#Remover $$$ Marcadores\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "#Remover espacios\n",
        "corpus_en = re.sub(r\" +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "#Limpieza de datos espaniol\n",
        "corpus_es = europarl_es\n",
        "corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n",
        "#Remover $$$ Marcadores\n",
        "corpus_es = re.sub(r\".\\$\\$\\$\", '', corpus_es)\n",
        "#Remover espacios\n",
        "corpus_es = re.sub(r\" +\", \" \", corpus_es)\n",
        "corpus_es = corpus_es.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph3_Jt0HXmGf"
      },
      "source": [
        "**Cambio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqUW2plV-pb4"
      },
      "outputs": [],
      "source": [
        "#Tokenizacion\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_es, target_vocab_size=2**13)\n",
        "\n",
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2\n",
        "\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1] for sentence in corpus_es]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSdWIx79-Geb"
      },
      "outputs": [],
      "source": [
        "#Removiendo oraciones largas\n",
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oHGfQDm9eT0"
      },
      "outputs": [],
      "source": [
        "#Crear INPUTS y OUTPUTS\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value = 0, padding = 'post', maxlen = MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value = 0, padding = 'post', maxlen = MAX_LENGTH)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jo7mP4uBAaH"
      },
      "outputs": [],
      "source": [
        "#Armando modelo\n",
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "     super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_models):\n",
        "    angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "    return pos * angles\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arrange(seq_length)[:, np.newaxis],\n",
        "                             np.arrange(d_model)[np.newaxis, :], d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0riqnlyQ8CI"
      },
      "outputs": [],
      "source": [
        "#Atenci√≥n\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product= tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8pZS3-F4-fw"
      },
      "outputs": [],
      "source": [
        "#SubCapa de atencion multiple\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape [-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size):\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splitted_inputs = tf.reshape(inputs, shape=shape)\n",
        "    return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(attention,\n",
        "                                  shape=(batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClKdJJo235RN"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate, name=\"encoder_layer\"):\n",
        "        super(EncoderLayer, self).__init__(name=name)\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # Atencion de subcabezas multiples\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_ffn = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_ffn = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "        attention = self.dropout(attention, training=training)\n",
        "        attention = self.norm(attention + inputs)\n",
        "\n",
        "        ffn_output = self.dense_1(attention)\n",
        "        ffn_output = self.dense_2(ffn_output)\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "        outputs = self.norm_ffn(ffn_output + attention)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZR9DQhl_l3s"
      },
      "outputs": [],
      "source": [
        "# Codificador (modificado por siaca)\n",
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate)\n",
        "                           for _ in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuaEVQhaLSUu"
      },
      "outputs": [],
      "source": [
        "#DecodificadorLayer\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "     super(DecoderLayer, self).__init__()\n",
        "     self.FFN_units = FFN_units\n",
        "     self.nb_proj = nb_proj\n",
        "     self.dropout_rate = dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    #Atencion de subcabezas multiples\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    #Atencion de subcabezas multiples + encoder output\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    #FFN\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                activation = \"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                            inputs,\n",
        "                                            inputs,\n",
        "                                            mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs,\n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUhMUmiLTTRk"
      },
      "outputs": [],
      "source": [
        "#Decodificador\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate)\n",
        "                           for i in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "          outputs = self.dec_layers[i](outputs,\n",
        "                                       enc_outputs,\n",
        "                                       mask_1,\n",
        "                                       mask_2,\n",
        "                                       training)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-oMPmEcVBDs"
      },
      "outputs": [],
      "source": [
        "#Transformador\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"Lin_output\")\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "      mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "      return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "      seq_len = tf.shape(seq)[1]\n",
        "      look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "      return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "      enc_mask = self.create_padding_mask(enc_inputs)\n",
        "      dec_mask_1 = tf.maximum(\n",
        "          self.create_padding_mask(dec_inputs),\n",
        "          self.create_look_ahead_mask(dec_inputs)\n",
        "      )\n",
        "      dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "      enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "      dec_outputs = self.decoder(dec_inputs,\n",
        "                                 enc_outputs,\n",
        "                                 enc_outputs,\n",
        "                                 dec_mask_1,\n",
        "                                 dec_mask_2,\n",
        "                                 training)\n",
        "\n",
        "      outputs = self.last_linear(dec_outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "k7LagWYLbfto",
        "outputId": "dba05185-24a6-45a4-8e71-9fbafcbedc2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Learning rate schedule 'CustomSchedule' must override `__call__(self, step)`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1b5462f99723>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomSchedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m optimizer = tf.keras.optimizers.Adam(learning_rate,\n\u001b[0m\u001b[1;32m     52\u001b[0m                                      \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                      \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mesh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;31m# For DTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;31m# Create a variable to hold the current learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 current_learning_rate = tf.convert_to_tensor(\n\u001b[0;32m--> 385\u001b[0;31m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 )\n\u001b[1;32m    387\u001b[0m                 self._current_learning_rate = tf.Variable(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/schedules/learning_rate_schedule.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;34mf\"Learning rate schedule '{self.__class__.__name__}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;34m\"must override `__call__(self, step)`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Learning rate schedule 'CustomSchedule' must override `__call__(self, step)`."
          ]
        }
      ],
      "source": [
        "#Entrenamiento\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Hyper-parametros\n",
        "\n",
        "D_MODEL = 128\n",
        "NB_LAYERS = 8\n",
        "FFN_UNITS = 512\n",
        "NB_PROJ = 8\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "loss_objetct = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                             reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_objetct(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracu = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def call(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYyoqaTWBvh5"
      },
      "outputs": [],
      "source": [
        "#Checkpoints\n",
        "# Define la ruta en Google Drive donde deseas almacenar los checkpoints\n",
        "drive_path = Path('/content/drive/MyDrive/Elementos_compartidos_Montesdeoca_Juan/Carlos_Mendoza-Juan_Montesdeoca/ckpt')\n",
        "\n",
        "# Crea el directorio si no existe\n",
        "drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Crea el objeto Checkpoint y el CheckpointManager\n",
        "checkpoint_path = drive_path / \"ckpt/\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, str(checkpoint_path), max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest Checkpoint Restored!!\")\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start of epoch{}\".format(epoch+1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for(batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss{:.4f} Accuracy {:.4f}\".format(\n",
        "          epoch+1, batch, train_loss.results(), train_accuracy.results()\n",
        "      ))\n",
        "  ckpt_save_path = ckpt.manager.save()\n",
        "  print(\"Saving checkpoints for epoch {} at {}\".format(epoch+1,\n",
        "                                                       ckpt_save_path))\n",
        "  print(\"Time taken for 1 epoch: {} secs \\n\".format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLeUPEwwSs_y"
      },
      "outputs": [],
      "source": [
        "#EVALUANDO MODELO\n",
        "\n",
        "def evaluate(inp_sentence):\n",
        "  inp_sentence = \\\n",
        "    [VOCAB_SIZE_EN-2] + tokenizer_en.enode(inp_sentence) + [VOCAB_SIZE_EN - 1]\n",
        "  enc_input = tf.expand.dims(inp_sentence, axis=0)\n",
        "\n",
        "  output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
        "\n",
        "  for _ in range (MAX_LENGTH):\n",
        "    predictions = transformer(enc_input, output, False)\n",
        "    prediction = predictions [:, -1:, :]\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.init32)\n",
        "\n",
        "    if predicted_id == VOCAB_SIZE_ES-1:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "\n",
        "  predicted_sentence = tokenizer_es.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_ES-2]\n",
        "  )\n",
        "\n",
        "  print(\"Input: {}\".format(sentence))\n",
        "  print(\"Predicted translations: {}\".format(predicted_sentence))\n",
        "\n",
        "translate(\"This is a really powerful tool!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}